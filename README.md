# TOPSIS on Pre-trained Models for Text Sentence Similarity

## ğŸ“Œ Assignment Description
This project applies the **TOPSIS (Technique for Order Preference by Similarity to Ideal Solution)** method to identify the **best pre-trained model for Text Sentence Similarity**.

The evaluation is performed using real textual data, multiple performance and efficiency criteria, and visual analysis. All results, plots, and rankings are included in this repository.

---

## ğŸ¯ Objective
To select the **most suitable pre-trained sentence similarity model** by considering:
- Semantic performance  
- Computational efficiency  

using a **multi-criteria decision-making (MCDM)** approach.

---
## ğŸ“‚ Repository Structure

```text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ model_evaluation_metrics.csv
â”‚   â””â”€â”€ topsis_final_ranking.csv
â”‚
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ topsis_scores.png
â”‚   â”œâ”€â”€ accuracy_comparison.png
â”‚   â”œâ”€â”€ inference_time.png
â”‚   â”œâ”€â”€ model_size_vs_accuracy.png
â”‚   â””â”€â”€ rank_vs_topsis_score.png
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ TopsisOnPretrainedModels.ipynb
â””â”€â”€ requirements.txt
```
---
## ğŸ¤– Pre-trained Models Evaluated
The following **five Sentence-Transformer models** were evaluated:

1. `all-MiniLM-L6-v2`  
2. `all-MPNet-base-v2`  
3. `paraphrase-MiniLM-L6-v2`  
4. `distilbert-base-nli-stsb-mean-tokens`  
5. `bert-base-nli-mean-tokens`  

These models represent a balance between lightweight and heavyweight architectures.

---

## ğŸ“Š Evaluation Criteria

| Criterion | Type | Description |
|---------|------|------------|
| Accuracy | Benefit â†‘ | Average cosine similarity between sentence pairs |
| Inference Time (ms) | Cost â†“ | Time required for similarity computation |
| Embedding Dimension | Cost â†“ | Dimensionality of sentence embeddings |
| Model Size (Proxy) | Cost â†“ | Approximate model complexity |

### Weights Used in TOPSIS

| Criterion | Weight |
|---------|--------|
| Accuracy | 0.40 |
| Inference Time | 0.20 |
| Embedding Dimension | 0.20 |
| Model Size | 0.20 |

---

## ğŸ“ˆ Results

### ğŸ”¹ Model Evaluation Metrics

All evaluation metrics are stored in:  
[data/model_evaluation_metrics.csv](data/model_evaluation_metrics.csv)

### ğŸ”¹ TOPSIS Final Ranking

The final TOPSIS scores and rankings are available in:  
[data/topsis_final_ranking.csv](data/topsis_final_ranking.csv)


**Best Performing Model:**  
âœ… **all-MiniLM-L6-v2**

This model achieved the highest TOPSIS score, indicating the best balance between performance and efficiency.

---

## ğŸ“Š Visual Analysis

All plots were generated programmatically and saved for reproducibility.

### ğŸ”¸ TOPSIS Score Comparison
![TOPSIS Scores](plots/topsis_scores.png)

---

### ğŸ”¸ Accuracy Comparison
![Accuracy Comparison](plots/accuracy_comparison.png)

---

### ğŸ”¸ Inference Time Comparison
![Inference Time](plots/inference_time.png)

---

### ğŸ”¸ Model Size vs Accuracy Trade-off
![Model Size vs Accuracy](plots/model_size_vs_accuracy.png)

---

### ğŸ”¸ Rank vs TOPSIS Score
![Rank vs TOPSIS Score](plots/rank_vs_topsis_score.png)

---

## ğŸ§  Methodology
1. Real textual data was collected using web scraping with a fallback mechanism.
2. Sentence pairs were created from the collected text.
3. Cosine similarity was used to evaluate semantic similarity.
4. A decision matrix was constructed using multiple criteria.
5. TOPSIS was applied to rank the models.
6. Results were visualized and exported as tables and plots.

---

## ğŸ Conclusion
Using TOPSIS for multi-criteria evaluation, **all-MiniLM-L6-v2** was identified as the most suitable pre-trained model for **Text Sentence Similarity**, offering an optimal balance between accuracy and computational efficiency.

---

## ğŸ› ï¸ How to Run the Project

### Install dependencies
```bash
pip install -r requirements.txt
```
### â–¶ï¸ Run the Notebook

Open the notebook and run all cells:

[TopsisOnPretrainedModels.ipynb](TopsisOnPretrainedModels.ipynb)
